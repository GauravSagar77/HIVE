Hive interview Questions

1. What is the definition of Hive? What is the present version of Hive? 
A1. Hive is a data warehouse system which is used to analyze structured data. It is built on the top of Hadoop. Its latest version is 4.0.0-alpha-2.

2. Is Hive suitable to be used for OLTP systems? Why?
A2. Hive doesn't support OLTP. Apache Hive is mainly used for batch processing i.e. OLAP and it is not used for OLTP because of the real-time operations of the database.

3. How is HIVE different from RDBMS? Does hive support ACID transactions. If not then give the proper reason.
A3. 
-RDBMS is used to maintain database, whereas Hive is used to maintain data warehouse.
-RDBMS uses SQL (Structured Query Language) whereas hive uses HQL (Hive Query Language).
-Schema is fixed in RDBMS whereas schema varies in Hive.
-In RDBMS, Normalized data is stored and in Hive-> Normalized and de-normalized both type of data is stored.
-RDBMS doesn’t support partitioning and Hive supports automation partition.
-Tables in rdms are sparse and Table in hive are dense.

Starting Version 0.14, Hive supports all ACID properties which enable us to use transactions, create transactional tables, and run queries like Insert, Update, and Delete on tables.

4. Explain the hive architecture and the different components of a Hive architecture?
A4. Hive Architecture step by step:-
Step-1: Execute Query – 
Interface of the Hive such as Command Line or Web user interface delivers query to the driver to execute. In this, UI calls the execute interface to the driver such as ODBC or JDBC. 
 
Step-2: Get Plan – 
Driver designs a session handle for the query and transfer the query to the compiler to make execution plan. In other words, driver interacts with the compiler. 
 
Step-3: Get Metadata – 
In this, the compiler transfers the metadata request to any database and the compiler gets the necessary metadata from the metastore. 
 
Step-4: Send Metadata – 
Metastore transfers metadata as an acknowledgment to the compiler. 
 
Step-5: Send Plan – 
Compiler communicating with driver with the execution plan made by the compiler to execute the query. 
 
Step-6: Execute Plan – 
Execute plan is sent to the execution engine by the driver. 
Execute Job
Job Done
Dfs operation (Metadata Operation)

Step-7: Fetch Results – 
Fetching results from the driver to the user interface (UI). 
 
Step-8: Send Results – 
Result is transferred to the execution engine from the driver. Sending results to Execution engine. 
When the result is retrieved from data nodes to the execution engine, it returns the result to the driver and to user interface (UI). 
-------------------------------------------------------------------------------------------------------------------------------------

The major components of Hive are described below: 

User Interface (UI) – 
As the name describes User interface provide an interface between user and hive. It enables user to submit queries and other operations to the system. Hive web UI, Hive command line, and Hive HD Insight (In windows server) are supported by the user interface. 
 
Hive Server – It is referred to as Apache Thrift Server. It accepts the request from different clients and provides it to Hive Driver.
Driver – 
Queries of the user after the interface are received by the driver within the Hive. Concept of session handles is implemented by driver. Execution and Fetching of APIs modelled on JDBC/ODBC interfaces is provided by the user. 
 
Compiler – 
Queries are parses, semantic analysis on the different query blocks and query expression is done by the compiler. Execution plan with the help of the table in the database and partition metadata observed from the metastore are generated by the compiler eventually. 

Metastore – 
All the structured data or information of the different tables and partition in the warehouse containing attributes and attributes level information are stored in the metastore. Sequences or de-sequences necessary to read and write data and the corresponding HDFS files where the data is stored. Hive selects corresponding database servers to stock the schema or Metadata of databases, tables, attributes in a table, data types of databases, and HDFS mapping. 
 
Execution Engine – 
Execution of the execution plan made by the compiler is performed in the execution engine. The plan is a DAG of stages. The dependencies within the various stages of the plan is managed by execution engine as well as it executes these stages on the suitable system components. 


5. Mention what Hive query processor does? And Mention what are the components of a Hive query processor?
A5. Hive query processor convert graph of MapReduce jobs with the execution time framework. So that the jobs can be executed in the order of dependencies.
    The major components of Apache Hive are the Hive clients, Hive services, Processing framework and Resource Management, and the Distributed Storage.

6. What are the three different modes in which we can operate Hive?
A6. Depending on the size of Hadoop data nodes, Hive can operate in two different modes:
    1.Local mode.
    2.Map-reduce mode.

7. Features and Limitations of Hive.
A7. 
Features:-
----------
1.Hive supports MapReduce, Tez, and Spark computing engine.
2.Hive is a stable batch-processing framework. It works as data warehouse.
3.Hive uses HIVE query language to query structure data which is easy to code.
4.HQL is a declarative language like SQL means it is non-procedural.
5.The table, the structure is similar to the RDBMS. It also supports partitioning and bucketing.
6.Partition, Bucket, and tables are the 3 data structures that hive supports.
7.It supports ETL.
8.Hive supports users to access files from HDFS, Apache HBase, Amazon S3, etc.
9.Hive is capable to process very large datasets of Petabytes in size.
10.We can easily embed custom MapReduce code with Hive to process unstructured data. 
11.Since we store Hive data on HDFS so fault tolerance is provided by Hadoop. 
12.JDBC/ODBC drivers are also available in Hive.
13.We can use a hive for data mining, predictive modeling, and document indexing.

Limitations:-
-------------
1.It doesn’t support online transaction processing (OLTP).
2.Subqueries are not supported.
3.The latency in the apache hive query is very high.
4.Hive is not used for real-time data querying since it takes a while to produce a result.
5.HQL does not support the Transaction processing feature.

8. How to create a Database in HIVE?
A8. Go to Hive shell by giving the command sudo hive and enter the command 'create database<data base name>' to create the new database in the Hive.

9. How to create a table in HIVE?
A9.
Managed table syntax==>

crate table <Table_name>
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '<delimiter>'
LINES TERMINATED BY '<delimiter>'
stored as '<fileFormat>'
--------------------------------
External table Syntax ===>

crate EXTERNAL table <Table_name>
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '<delimiter>' 
LINES TERMINATED BY '<delimiter>'
stored as '<fileFormat>'
LOCATION '<HDFS_locan>'   

10.What do you mean by describe and describe extended and describe formatted with respect to database and table?
A10.
-describe database shows/displays the name of the database, its comment and the root location on the file system & describe table shows/displays the list of columns (including partition columns) for the given table.
-describe extended - This will show table columns, data types, and other details of the table. Other details will be displayed in single line. 
-describe formatted - This will show table columns, data types, and other details of the table. Other details will be displayed into multiple lines. 

11.How to skip header rows from a table in Hive?
A11. While table creation, give this below syntax.
hive > tblproperties("skip.header.line.count"="1");

12.What is a hive operator? What are the different types of hive operators?
A12. 
-Hive provides various Built-in operators for data operations to be implemented on the tables present inside Apache Hive warehouse. Hive operators are used for mathematical operations on operands. It returns specific value as per the logic applied.
-Hive operators are as below:-
Operators Precedences
Relational Operators
Arithmetic Operators
Logical Operators
String Operators
Complex Type Constructors
Operators on Complex Types

14. Write hive DDL and DML commands.
A14.
- Hive DDL commands are the statements used for defining and changing the structure of a table or database in Hive. It is used to build or modify the tables and other objects in the database.The several types of Hive DDL commands are:
CREATE
SHOW
DESCRIBE
USE
DROP
ALTER
TRUNCATE

- Hive DML (Data Manipulation Language) commands are used to insert, update, retrieve, and delete data from the Hive table once the table and database schema has been defined using Hive DDL commands.The various Hive DML commands are:
LOAD
SELECT
INSERT
DELETE
UPDATE
EXPORT
IMPORT

15.Explain about SORT BY, ORDER BY, DISTRIBUTE BY and CLUSTER BY in Hive.
A15. The SORT BY and ORDER BY clauses are used to define the order of the output data whereas DISTRIBUTE BY and CLUSTER BY clauses are used to distribute the data to multiple reducers based on the key columns.

16.Difference between "Internal Table" and "External Table" and Mention when to choose “Internal Table” and “External Table” in Hive?
A.16
1. An internal table data is stored in the warehouse folder, whereas an external table data is stored at the location you mentioned in table creation.
2. Dropping an internal table deletes the table metadata from Metastore and also removes all its data/files from HDFS. 
   Dropping an external table, just drop the metadata of the table from Metastore and keeps the actual data as-is on HDFS location.
3. Create table tablename(---> for Internal table) & create EXTERNAL table tablename (---> for External table),Also we have to give the location.

-We create an external table for external use as when we want to use the data outside the Hive.
-Use internal tables when: The data is temporary.

17.Where does the data of a Hive table get stored?
A17.  Hive data are stored in one of Hadoop compatible filesystem: S3, HDFS or other compatible filesystem.
	Hive metadata are stored in RDBMS like MySQL, see supported RDBMS.

18.Is it possible to change the default location of a managed table?
A18. Yes, by using the clause – LOCATION '<hdfs_path>', we can change the default location of a managed table.

19.What is a metastore in Hive? What is the default database provided by Apache Hive for metastore?
A19.  Hive Metastore is a component in Hive that stores the catalog of the system that contains the metadata about Hive create columns, Hive table creation, and partitions. 
	Derby is the default database for the embedded metastore.

20.Why does Hive not store metadata information in HDFS?
A20.  Hive stores metadata information in the metastore using RDBMS instead of HDFS. 
	The reason for choosing RDBMS is to achieve low latency as HDFS read/write operations are time consuming processes.

21.What is a partition in Hive? And Why do we perform partitioning in Hive?
A21.  The partitioning in Hive means dividing the table into some parts based on the values of a particular column like date, course, city or country. 
	The advantage of partitioning is that since the data is stored in slices, the query response time becomes faster.

22.What is the difference between dynamic partitioning and static partitioning?
A22. 	Static  ===> We have to manually specify partition columns. Human errors are quite common.
	Dynamic ===> Instead of manually loading data into hive tables with partitions, we are instrucing hive to load data from basetable.

23.How do you check if a particular partition exists?
A23.SHOW PARTITIONS table_name 
    PARTITION(partitioned_column=’partition_value’)

24.How can you stop a partition form being queried?
A24.  By using the ENABLE OFFLINE clause with ALTER TABLE statement.

25.Why do we need buckets? How Hive distributes the rows into buckets?
A25. When there is more unique data(thats High Cardinality), we use bucketing.

26.In Hive, how can you enable buckets?
A26.partition=true property. So, we can enable dynamic bucketing while loading data into hive table By setting this property.

27.How does bucketing help in the faster execution of queries?
A27.Bucketing in Hive entails the decomposition of a table data set into smaller parts. Thus, data is easier to handle. With bucketing, we join similar data types and write them to a single file. This step here greatly enhances performance while joining tables or reading data.

28.How to optimise Hive Performance? Explain in very detail.
A28.

29. What is the use of Hcatalog?
A29.HCatalog is a tool that allows you to access Hive metastore tables within Pig, Spark SQL, and/or custom MapReduce applications.

30. Explain about the different types of join in Hive.
A30.
a. Inner Join
Basically, to combine and retrieve the records from multiple tables we use Hive Join clause. Moreover, in SQL JOIN is as same as OUTER JOIN. Moreover, by using the primary keys and foreign keys of the tables JOIN condition is to be raised.

b. Left Outer Join
On defining HiveQL Left Outer Join, even if there are no matches in the right table it returns all the rows from the left table.
To be more specific, even if the ON clause matches 0 (zero) records in the right table, then also this Hive JOIN still returns a row in the result. Although, it returns with NULL in each column from the right table.

c. Right Outer Join
Basically, even if there are no matches in the left table, HiveQL Right Outer Join returns all the rows from the right table.
To be more specific, even if the ON clause matches 0 (zero) records in the left table, then also this Hive JOIN still returns a row in the result. Although, it returns with NULL in each column from the left table
In addition, it returns all the values from the right table. Also, the matched values from the left table or NULL in case of no matching join predicate.

d. Full Outer Join
The major purpose of this HiveQL Full outer Join is it combines the records of both the left and the right outer tables which fulfills the Hive JOIN condition. Moreover, this joined table contains either all the records from both the tables or fills in NULL values for missing matches on either side.


Q31.Is it possible to create a Cartesian join between 2 tables, using Hive?
Ans:-   Yes it is possible to create a Cartesian join between 2 tables, using Hive.
How to use a cross join in Hive
Cross join, also known asCartesian product, is a way of joining multiple tables in which all the rows or tuples from one table are paired with the rows and tuples from another table. For example, if the left-hand side table has 10 rows and the right-hand side table has 13 rows then the result set after joining the two tables will be 130 rows. That means all the rows from the left-hand side table (having 10 rows) are paired with all the tables from the right-hand side table (having 13 rows).

Q32.Explain the SMB Join in Hive?
Ans:-   SMB Join- SMB is a join performed on bucket tables that have the same sorted, bucket, and join condition columns. It reads data from both bucket tables and performs common joins (map and reduce triggered) on the bucket tables.
Q33.What is the difference between order by and sort by which one we should use?
Ans:-    ORDER BY performs a total ordering of the query result set. This means that all the data is passed through a single reducer, which may take an unacceptably long time to execute for larger data sets.
SORT BY orders the data only within each reducer, thereby performing a local ordering, where each reducer’s output will be sorted. You will not achieve a total ordering on the dataset. Better performance is traded for total ordering.
Assume you have a sales table in a company and it has sales entries from salesman around the globe. How do you rank each salesperson by country based on their sales volume in Hive?
Hive support several analytic functions and one of the functions is RANK() and it is designed to do this operation.
Hive>SELECT
rep_name, rep_country, sales_volume,
rank() over (PARTITION BY rep_country ORDER BY sales_volume DESC) as rank
FROM
salesrep;

Hive supports  SORT BY which sorts the data per reducer. The difference between "order by" and "sort by" is that the former guarantees total order in the output while the latter only guarantees ordering of the rows within a reducer. If there are more than one reducer, "sort by" may give partially ordered final results.

Q34.What is the usefulness of the DISTRIBUTE BY clause in Hive?
Ans:-    DISTRIBUTE BY clause is used to distribute the input rows among reducers. It ensures that all rows for the same key columns are going to the same reducer. So, if we need to partition the data on some key column, we can use the DISTRIBUTE BY clause in the hive queries.

Q35. How does data transfer happen from HDFS to Hive?
Ans:-   To query data in HDFS in Hive,you apply a schema to the data and then store data in ORC format.Incrementally update the imported data.
Updating imported tables involves importing incremental changes made to the original table using sqoop and then merging changes with the tables imported into Hive.

Q36.Wherever (Different Directory) I run the hive query, it creates a new metastore_db, please explain the reason for it? 
Ans:-    Basically, it creates the local metastore, while we run the hive in embedded mode. Also, it looks whether metastore already exist or not before creating the metastore. Hence, in configuration file hive-site.xml. Property is “javax.jdo.option.ConnectionURL” with default value “jdbc:derby:;databaseName=metastore_db;create=true” this property is defined. Hence, to change the behavior change the location to the absolute path, thus metastore will be used from that location.
37.What will happen in case you have not issued the command: ‘SET hive.enforce.bucketing=true;’ before bucketing a table in Hive?
Ans:-   The command:  ‘SET hive.enforce.bucketing=true;’ allows one to have the correct number of reducer while using ‘CLUSTER BY’ clause for bucketing a column. In case it’s not done, one may find the number of files that will be generated in the table directory to be not equal to the number of buckets. As an alternative, one may also set the number of reducer equal to the number of buckets by using set mapred.reduce.task = num_bucket.

Q38.Can a table be renamed in Hive? 
Ans:-    You can rename the table name in the hive. You need to use the alter command. This command allows you to change the table name as shown below.
$ ALTER TABLE name RENAME TO new_name

Q39.Write a query to insert a new column(new_col INT) into a hive table at a position before an existing column (x_col)
Ans:-    ALTER TABLE table_name
CHANGE COLUMN new_col
INT BEFORE x_col;

Q40.What is serde operation in HIVE?
Ans:-    SerDe means Serializer and Deserializer. Hive uses SerDe and FileFormat to read and write table rows. Main use of SerDe interface is for IO operations. A SerDe allows hive to read the data from the table and write it back to the HDFS in any custom format. If we have unstructured data, then we use RegEx SerDe which will instruct hive how to handle that record.We can also write our own Custom SerDe in any format. Let us see the definition of Serializer and Deserailizer.
Deserailizer
Deserializer is conversion of string or binary data into java object when we any submit any query.
Serailizer
Serializer converts java object into string or binary object. It is used when writing the data such as insert- select statements.Hive currently uses these FileFormat classes to read and write HDFS files:
⦁	TextInputFormat/HiveIgnoreKeyTextOutputFormat: These 2 classes read/write data in plain text file format.
⦁	SequenceFileInputFormat/SequenceFileOutputFormat: These 2 classes read/write data in Hadoop SequenceFile format.

41.Explain how Hive Deserializes and serialises the data?
A41. Hive uses the SerDe interface for IO. The interface handles both serialization and deserialization and also interpreting the results of serialization as individual fields for processing. A SerDe allows Hive to read in data from a table, and write it back out to HDFS in any custom format.

42.Write the name of the built-in serde in hive.
A42. 
-MetadataTypedColumnsetSerDe
-LazySimpleSerDe
-Thrift SerDe
-Dynamic SerDe

43.What is the need of custom Serde?
A43. It allows Hive to read in data from a table, and write it back out to HDFS in any custom format. Anyone can write their own SerDe for their own data formats. 

44.Can you write the name of a complex data type(collection data types) in Hive?
A44. Array, Map, Struct and union.

45.Can hive queries be executed from script files? How?
A45. Yes. We can execute Hive queries from the script files using the source command.

46.What are the default record and field delimiter used for hive text files?
A46. The default record delimiter is − \n
     And the filed delimiters are − \001,\002,\003

47.How do you list all databases in Hive whose name starts with s?
A47. SHOW DATABASES LIKE ‘s.*’

48.What is the difference between LIKE and RLIKE operators in Hive?
A48.We use LIKE to search for string with similar text. 
    RLIKE (Right-Like) is a special function in Hive where if any substring of A matches with B then it evaluates to true. 

49.How to change the column data type in Hive?
A49. By using ALTER command.
ALTER TABLE table_name CHANGE column_name column_name new_datatype;

50.How will you convert the string ’51.2’ to a float value in the particular column?
A50. By using the cast function to cast the type from one to another.
     CAST(from_datatype AS to_datatype)

51.What will be the result when you cast ‘abc’ (string) as INT?
A51. It will return NULL.

52.What does the following query do?
a. INSERT OVERWRITE TABLE employees
b. PARTITION (country, state)
c. SELECT ..., se.cnty, se.st
d. FROM staged_employees se;

A52. It will give all serialized country and serialized state and it is partition by only country and state from staged_employees serde table 

53.Write a query where you can overwrite data in a new table from the existing table.
A53.hive> INSERT OVERWRITE TABLE New_Table select * from Existing_Table;

54.What is the maximum size of a string data type supported by Hive? Explain how Hive supports binary formats.
A54. Max size=255
Hive supports two more primitive data types, BOOLEAN and BINARY.
Binary is a sequence of bytes. It is similar to the VARBINARY data type found in many relational databases.
	
55. What File Formats and Applications Does Hive Support?
A55. Hive supports File formats like comma-separated value (. csv) TextFile, RCFile, ORC, and Parquet and applications like Java,PHP,Python,C++,Ruby.

56.How do ORC format tables help Hive to enhance its performance?
A56.Using the ORC format leads to a reduction in the size of the data stored, as this file format has high compression ratios. As the data size is reduced, 
the time to read and write the data is also reduced. The ORC format improves query performance also by the way it stores data in a file.

57.How can Hive avoid mapreduce while processing the query?
A57. Hive avoid MapReduce to return query results by setting the hive.exec.mode.

58.What is view and indexing in hive?
A58.  Views are generated based on user requirements. You can save any result set data as a view.
	Indexes are a pointer or reference to a record in a table as in relational databases. Indexes facilitate in making query execution or search operation faster.

59.Can the name of a view be the same as the name of a hive table?
A59. The name of a view must be unique, and it cannot be the same as any table or database or view's name.

60.What types of costs are associated in creating indexes on hive tables?
A60.Basically, there is a processing cost in arranging the values of the column on which index is created since Indexes occupies.

61.Give the command to see the indexes on a table.
A61. To see the index for a specific table use SHOW INDEX: SHOW INDEX FROM table; 

62. Explain the process to access subdirectories recursively in Hive queries.
A62. We can use following commands in Hive to recursively access sub-directories:
hive> Set mapred.input.dir.recursive=true;
hive> Set hive.mapred.supports.subdirectories=true;
Once above options are set to true, Hive will recursively access sub-directories of a directory in MapReduce.

63.If you run a select * query in Hive, why doesn't it run MapReduce?
A63.Hive requires a map-reduce job since it needs to extract the 'column' from each row by parsing it from the file it loads.

64.What are the uses of Hive Explode?
A64. The explode function explodes an array to multiple rows. Returns a row-set with a single column (col), one row for each element from the array.

65. What is the available mechanism for connecting applications when we run Hive as a server?
A65.The mechanism is done by following the below steps:
Thrift client: By using thrift client, we can call Hive commands from different programming languages such as Java, Python, C++, Ruby
JDBC driver: It enables accessing data and supports Type 4 JDBC driver
ODBC driver: ODBC API Standards apply for the Hive DBMS. It supports ODBC protocols.

66.Can the default location of a managed table be changed in Hive?
A66.Yes, we can do it by using the clause – LOCATION '<hdfs_path>' we can change the default location of a managed table.

67.What is the Hive ObjectInspector function?
A67.Hive ObjectInspector is a group of flexible APIs to inspect value in different data representation

68.What is UDF in Hive?
A68. What is a UDF in Hive?
In Hive, the users can define own functions to meet certain client requirements. These are known as UDFs in Hive.

69.Write a query to extract data from hdfs to hive.
A69.
INSERT OVERWRITE DIRECTORY "HDFS_Path" ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' SELECT * FROM Table_name LIMIT 10;

70.What is TextInputFormat and SequenceFileInputFormat in hive.
A70. These are fifferent File FORMATS of MAPREDUCE.
     TextInputFormat ==>Default inputformat class

71.How can you prevent a large job from running for a long time in a hive?
A71. We can do it by setting the MapReduce jobs to execute in strict mode set hive.

72.When do we use explode in Hive?
A72.  Lateral View Explode is another function in Hive that is used to split a column, but instead of creating multiple rows, it creates multiple columns. 
	This function is beneficial when working with maps. 
	It allows us to split a map column into multiple columns, each containing one key-value pair from the map.

73.Can Hive process any type of data formats? Why? Explain in very detail
A73. Hive supports four file formats those are TEXTFILE, SEQUENCEFILE, ORC and RCFILE (Record Columnar File).
For single user metadata storage, Hive uses derby database and for multiple user Metadata or shared Metadata case Hive uses MYSQL.

74.Whenever we run a Hive query, a new metastore_db is created. Why?
A74. The property of interest here is javax.jdo.option.ConnectionURL. 
The default value of this property is jdbc:derby:;databaseName=metastore_db;create=true. 
This value specifies that you will be using embedded derby as your Hive metastore and the location of the metastore is metastore_db. 

75.Can we change the data type of a column in a hive table? Write a complete query.
A75. ALTER TABLE table_name CHANGE column_name column_name new_datatype;

76.While loading data into a hive table using the LOAD DATA clause, how do you specify it is a hdfs file and not a local file ?
A76.  hive> load data inpath 'HDFS_Path' into table table_name; ==>> This command load data from HDFS path
	hive> load data local inpath 'local_file_path' into table table_name; ==>> This command load data from LFS path

77.What is the precedence order in Hive configuration?
A77. We are using a precedence hierarchy for setting properties: The SET command in Hive. The command-line –hiveconf option.

78.Which interface is used for accessing the Hive metastore?
A78. WebHCat API web interface

79.Is it possible to compress json in the Hive external table ?
A79. Yes. We have to gzip the files and put them as is (*.gz) into the table location

80.What is the difference between local and remote metastores?
A80. 
Local Metastore:- Here metastore service still runs in the same JVM as Hive but it connects to a database running in a separate process either on same machine
 or on a remote machine.      Remote Metastore:- Metastore runs in its own separate JVM not on hive service JVM.

81.What is the purpose of archiving tables in Hive?
A81. Hadoop archives are used as an approach to reduce the number of files in partitions. Apache Hive has built-in support for converting files on existing
 partitions to a Hadoop Archive. In this manner, a partition that would have once consisted of hundreds of files can be allowed to occupy just around 3 files.
 This number can vary depending on the settings. The trade-off in such a case is that the queries may end up having a higher latency, caused by the additional 
 overhead of reading from Hadoop Archive. Archiving does not compress the files, but works similar to the Linux ‘tar’ command.

Q82.What is DBPROPERTY in Hive?
Ans:-    The DB properties are nothing but mentioning the details about the database created by the user. Suppose the name of the user, the type of the database
 and the tables it has, the date on which the database is created etc. This makes the other user easy the recognize the database and use it according to the 
 requirement.

83.Differentiate between local mode and MapReduce mode in Hive.
A83. :-    Local mode is actually a local simulation of MapReduce in Hadoop’s LocalJobRunner class.
MapReduce mode (also known as Hadoop mode): Pig is executed on the Hadoop cluster. In this case, the Pig Script gets converted into a series of MapReduce jobs
 that are then run on the Hadoop cluster.












