Que: Will the reducer work or not if you use “Limit 1” in any HiveQL query?

Ans: In Hive, the use of "LIMIT 1" in a query would cause the reducer phase to be skipped. This is because the "LIMIT 1" clause effectively limits the results of the query to a 
single row, which can be returned directly from the mapper without the need for a reducer. As a result, the data is not passed to the reducer phase and no aggregation or processing 
takes place in this stage.


Que: Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 

Ans: The default metastore configuration allows only one Hive session to be opened at a time for accessing the metastore.   
 Therefore, if multiple clients try to access the metastore at the same time, they will get an error. One has to use a standalone metastore,   
 i.e. Local or remote metastore configuration in Apache Hive for allowing access to multiple clients concurrently.   


Que: Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?

Ans The "month" field may be used to split the "transaction details" table. Based on the value of the partition column, partitioning divides the data in the table into distinct
 subdirectories. Due to Hive's ability to process just the partitions that are pertinent to the query, less data must be processed, which improves query performance. The information
 kept in the "transaction details" database can be compressed. Data size reduction by compression makes data more readily readable from disc, which can enhance query performance. 
 The "month" field of the "transaction details" database can be used to group data. As Hive can only scan the pertinent buckets, bucketing separates comparable records into distinct 
 files, which can increase query speed.


Que: How can you add a new partition for the month December in the above partitioned table?

Ans: ALTER TABLE transaction_details ADD PARTITION (month='December');


Que: I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?

Ans: Execute this command : set hive.exec.dynamic.partition.mode=nonstrict;


Que: Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?

Ans: Create a table in Hive that maps to the structure of the CSV file, and then load the data from
 the file into the table using the LOAD DATA INPATH command.
   Ex. load data inpath '/tmp/sample.csv' into table tmp_table;


Que: Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?

Ans: Solution to this problem is to use the Hive's "external table" feature to combine the small CSV files into a single Hive table without copying the data into the Hive warehouse.
 This way, you can avoid degrading the performance of the system while still being able to access the data as if it were stored in a single Hive table.


Que: LOAD DATA LOCAL INPATH ‘Home/country/state/’
     OVERWRITE INTO TABLE address;

     The following statement failed to execute. What can be the cause?

Ans: The local inpath should contain a file and not a directory. 


Que: Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?

Ans es, we can add the nodes by following the below steps:  
   
 Step 1: Take a new system; create a new username and password  
 Step 2: Install SSH and with the master node setup SSH connections  
 Step 3: Add ssh public_rsa id key to the authorized keys file  
 Step 4: Add the new DataNode hostname, IP address, and other details in /etc/hosts slaves file:  
 Step 5: Start the DataNode on a new node and Login to the new node like hadoop 
 Step 6: Start HDFS of the newly added slave node, and Check the output of the jps command on the new node 




Hive Practical questions:

Que: Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)

Ans: # Creating tables:

    CREATE TABLE CUSTOMERS
    (
    ID INT,
    NAME STRING,
    AGE INT,
    ADDRESS STRING,
    SALARY FLOAT
    )
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ',';

    CREATE TABLE ORDERS
    (
     OID INT,
     DATE STRING,
     CUSTOMER_ID INT,
     AMOUNT FLOAT
    )
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY ',';
    
    # Now perform different joins operations on top of these tables
    
    SELECT * FROM CUSTOMERS cI NNER JOIN ORDERS o ON c.ID = o.CUSTOMER_ID;

    SELECT * FROM CUSTOMERS c LEFT JOIN ORDERS o ON c.ID = o.CUSTOMER_ID;

    SELECT * FROM CUSTOMERS c RIGHT OUTER JOIN ORDERS o ON c.ID = o.CUSTOMER_ID;

    SELECT * FROM CUSTOMERS c LEFT OUTER JOIN ORDERS o ON c.ID = o.CUSTOMER_ID;
 

1. Create a hive table as per given schema in your dataset.
    Create  table Air_Quality
    (
    `Date` date,
    `time` string,
    co float,
    s1_co int,
    nmhc int,
    C6h6 float,
    S2_nmhc int,
    Nox int,
    S3_nox int,
    No2 int,
    S4_no2 int,
    s5_o3 int,
    T float,
    RH float,
    AH float
    )
    row format delimited
    fields terminated by ';'
    tblproperties ("skip.header.line.count" = "1");
    location ' /user/Air_Quality_data'

2. Try to place a data into table location.
    load data local inpath '/config/workspace' into table Air_Quality;
    
3. Perform a select operation.
    select `Date`, No2 from Air_Quality;
    
4. Fetch the result of the select operation in your local as a csv file.
    insert overwrite local directory ' /config/workspace' row format delimited fields terminated by ',' select * from Air_Quality;

5. Perform group by operation.
    select `Date`, avg(No2) from Air_Quality group by `Date`;

7. Perform filter operation at least 5 kinds of filter examples.
    select * from Air_Quality where No2 > 77;
	select Date from Air_Quality where T=9.7;
	select Date,time as Date_Time from Air_Quality where co>1 limit 5;
	select RH from Air_Quality where no2!=-200 limit 5;
	select Date,time,Nox from Air_Quality where Nox like '9%' limit 5;

	
8.  show and example of regex operation
    select T,regexp_extract(Date,"(-2004*)",1) as xyz from Air_Quality limit 5;

9. Alter table operation.
    alter table Air_Quality rename Air_Quality_data;

10. Drop table operation.
    drop table Air_Quality;
    
12. Order by operation.
    select *  from Air_Quality  order by No2 asc;
    
13. where clause operations you have to perform.
    select * from Air_Quality where `Date`='10/03/2004';
    
14. Sorting operation you have to perform. 
    select  C6h6, `Date`  from Air_Quality  order by C6h6 desc;

15. Distinct operation you have to perform. 
    select distinct(Date) from Air_Quality;

16. Like an operation you have to perform.
    select * from Air_Quality where co like '3%' limit 5;

17. union operation you have to perform .	
	select distinct(dates),round(t) as table_1 from air_quality_uci 
	union all 
	select distinct(dates),round(rh) as table_2 from air_quality_uci limit 5;
    
18. Table view operation you have to perform.
    create view Air_Quality_data as select no, Co, s5, `Date`, time from Air_Quality;


